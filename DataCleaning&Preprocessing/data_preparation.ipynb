{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009d0129",
   "metadata": {},
   "source": [
    "First dataset : Data-Science and AI Jobs – Indeed\n",
    "Source : Kaggle (Licence CC BY-SA 4.0)\n",
    "Nombre d’annonces : ~17 000\n",
    "Lien : https://www.kaggle.com/datasets/srivnaman/data-science-and-ai-jobsindeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e4fe244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu du Dataset 1 (5 premières lignes):\n",
      "                                           Job Title  \\\n",
      "0  newArtificial intelligence security specialist...   \n",
      "1                              newEDA Tool Developer   \n",
      "2  Junior Project Engineer - Artificial Intellige...   \n",
      "3                                          Test Lead   \n",
      "4                                     + Lead Testers   \n",
      "\n",
      "                                         Description Location Date Company  \\\n",
      "0  An artificial intelligence (AI) specialist app...    India  NaT           \n",
      "1  We are seeking highly motivated individuals wi...    India  NaT           \n",
      "2  We are dedicated to providing quality and effe...  Unknown  NaT           \n",
      "3  Working on over 18 different browser/os/ mobil...  Unknown  NaT           \n",
      "4  Total Experience: 6- 8 yrs.\\nAutomation Testin...    India  NaT           \n",
      "\n",
      "  Salary URL Skills  \n",
      "0   None       None  \n",
      "1   None       None  \n",
      "2   None       None  \n",
      "3   None       None  \n",
      "4   None       None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Traitement du Dataset 1\n",
    "def traiter_dataset1():\n",
    "    # Chargement\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science and AI Jobs – Indeed\\DataScience and AI Jobs.csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # Vérification colonnes\n",
    "    colonnes_requises = [\"title\", \"location\", \"summary\", \"salary\"]\n",
    "    for col in colonnes_requises:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Colonne manquante : {col}\")\n",
    "    \n",
    "    # Transformation\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['title'],\n",
    "        'Description': df['summary'],\n",
    "        'Location': df['location'],\n",
    "        'Date': pd.NaT,\n",
    "        'Company': '',\n",
    "        'Salary': None,\n",
    "        'URL': '',\n",
    "        'Skills': None\n",
    "    })\n",
    "    \n",
    "    # Ajout pays\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        if any(mot in loc for mot in [\"india\", \"delhi\", \"mumbai\", \"hyderabad\", \"bengaluru\"]):\n",
    "            return \"India\"\n",
    "        if any(mot in loc for mot in [\"remote\", \"hybrid\"]):\n",
    "            return \"India\"\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# 2. Exécution et affichage\n",
    "dataset1 = traiter_dataset1()\n",
    "print(\"\\nAperçu du Dataset 1 (5 premières lignes):\")\n",
    "print(dataset1.head())\n",
    "\n",
    "# 3. Stockage en mémoire pour concaténation future\n",
    "datasets_traites = {\"dataset1\": dataset1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637865f6",
   "metadata": {},
   "source": [
    "second dataset : Data-Science, Data-Analyst & ML Jobs – Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/mdwaquarazam/data-science-dataanalyst-and-ml-job-indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "717eb968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu du Dataset 2 (5 premières lignes):\n",
      "                                           Job Title  \\\n",
      "0  Technology Lead : Data Science I Machine Learning   \n",
      "1                Software Engineer, Machine Learning   \n",
      "2        Experienced Over The Shoulder Mechanic- C17   \n",
      "3                                     Data Scientist   \n",
      "4                                   Python Developer   \n",
      "\n",
      "                                         Description Location       Date  \\\n",
      "0  A day in the life of an Infoscion • As part of...    India 2022-06-26   \n",
      "1  2 years of relevant work experience in machine...    India 2022-06-26   \n",
      "2  This position will focus on supporting the Boe...    India 2022-07-23   \n",
      "3  Anchor ML development track in a client projec...    India 2022-06-26   \n",
      "4  Should have a decent understanding of the Mach...    India 2022-06-26   \n",
      "\n",
      "           Company Salary                                                URL  \\\n",
      "0  Infosys Limited   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "1           Google   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "2           BOEING   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "3  Infosys Limited   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "4  Infosys Limited   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "\n",
      "  Skills  \n",
      "0   None  \n",
      "1   None  \n",
      "2   None  \n",
      "3   None  \n",
      "4   None  \n",
      "\n",
      "Répartition par pays:\n",
      "Location\n",
      "India      1580\n",
      "Unknown       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import re\n",
    "\n",
    "# 1. Fonction de traitement pour le Dataset 2\n",
    "def traiter_dataset2():\n",
    "    # Chargement des données\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science, Data-Analyst & ML Jobs – Indeed\\job_dataset.csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # Nettoyage des noms de colonnes\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Vérification des colonnes requises\n",
    "    colonnes_requises = [\"job_title\", \"company\", \"job_location\", \"job_summary\", \n",
    "                        \"post_date\", \"today\", \"job_salary\", \"job_url\"]\n",
    "    for col in colonnes_requises:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Colonne manquante : {col}\")\n",
    "    \n",
    "    # Conversion des dates\n",
    "    def convertir_date(post_date_str, reference_date):\n",
    "        if pd.isna(post_date_str):\n",
    "            return pd.NaT\n",
    "            \n",
    "        post_date_str = str(post_date_str)\n",
    "        \n",
    "        # Gestion des cas particuliers\n",
    "        if post_date_str in [\"Hiring ongoing\", \"PostedJust posted\", \"PostedToday\"]:\n",
    "            return reference_date\n",
    "        if post_date_str == \"PostedYesterday\":\n",
    "            return reference_date - timedelta(days=1)\n",
    "            \n",
    "        # Extraction du nombre de jours\n",
    "        jours = re.search(r'(\\d+)\\+? days? ago', post_date_str)\n",
    "        if jours:\n",
    "            return reference_date - timedelta(days=int(jours.group(1)))\n",
    "            \n",
    "        return pd.to_datetime(post_date_str, errors='coerce')\n",
    "    \n",
    "    reference_date = pd.to_datetime(df['today'].iloc[0])\n",
    "    df['date_calculee'] = df['post_date'].apply(lambda x: convertir_date(x, reference_date))\n",
    "    \n",
    "    # Création du DataFrame standardisé\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['job_title'],\n",
    "        'Description': df['job_summary'],\n",
    "        'Location': df['job_location'],\n",
    "        'Date': df['date_calculee'],\n",
    "        'Company': df['company'],\n",
    "        'Salary': None,\n",
    "        'URL': df['job_url'],\n",
    "        'Skills': None\n",
    "    })\n",
    "    \n",
    "    # Ajout de la colonne Country\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        return \"Unknown\" if \"unknown\" in loc else \"India\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# 2. Exécution et affichage\n",
    "dataset2 = traiter_dataset2()\n",
    "print(\"\\nAperçu du Dataset 2 (5 premières lignes):\")\n",
    "print(dataset2.head())\n",
    "print(\"\\nRépartition par pays:\")\n",
    "print(dataset2['Location'].value_counts())\n",
    "\n",
    "# 3. Stockage en mémoire\n",
    "datasets_traites[\"dataset2\"] = dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162feb5d",
   "metadata": {},
   "source": [
    "third dataset : Data-Science Jobs & Salaries – Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/ritiksharma07/data-science-jobs-andsalaries-indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9222e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu du Dataset 3 (5 premières lignes):\n",
      "                                           Job Title  \\\n",
      "0                                     Data Scientist   \n",
      "1  Senior Artificial Intelligence Researcher for ...   \n",
      "2                              Senior Data Scientist   \n",
      "3                                     Data Scientist   \n",
      "4                                     Data Scientist   \n",
      "\n",
      "                                         Description       Location  Date  \\\n",
      "0  The ideal candidate should be highly skilled i...  United States  <NA>   \n",
      "1  As a member of our research team, you will con...        Unknown  <NA>   \n",
      "2  Utilize machine learning techniques, algorithm...        Unknown  <NA>   \n",
      "3  Conduct ad-hoc analyses, build robust automate...        Unknown  <NA>   \n",
      "4  Develops and maintains analytics product proje...  United States  <NA>   \n",
      "\n",
      "                                          Company                      Salary  \\\n",
      "0                                     Robert Half  $120,000 - $140,000 a year   \n",
      "1  Johns Hopkins Applied Physics Laboratory (APL)                         NaN   \n",
      "2               Modern Technology Solutions, Inc.                         NaN   \n",
      "3                        Twitch Interactive, Inc.                         NaN   \n",
      "4               US Office of Personnel Management  $103,409 - $167,336 a year   \n",
      "\n",
      "                                                 URL Skills  \n",
      "0  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   None  \n",
      "1  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   None  \n",
      "2  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   None  \n",
      "3  https://www.indeed.com/rc/clk?jk=4593a41913786...   None  \n",
      "4  https://www.indeed.com/rc/clk?jk=d393197b7bc42...   None  \n",
      "\n",
      "Répartition par pays:\n",
      "Location\n",
      "Unknown          138\n",
      "United States     62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Statistiques des valeurs manquantes:\n",
      "- Dates: 200/200\n",
      "- Compétences: 200/200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def traiter_dataset3():\n",
    "    # Chargement des données\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science Jobs & Salaries – Indeed\\Indeed-Data Science Jobs List.csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # Vérification des colonnes requises\n",
    "    colonnes_requises = {'Job Title', 'Company', 'Location', 'Salary', \n",
    "                        'Short Description', 'Posted At', 'Job link'}\n",
    "    colonnes_manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "    if colonnes_manquantes:\n",
    "        raise ValueError(f\"Colonnes manquantes : {colonnes_manquantes}\")\n",
    "    \n",
    "    # Création du DataFrame standardisé\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['Job Title'],\n",
    "        'Description': df['Short Description'],\n",
    "        'Location': df['Location'],\n",
    "        'Date': pd.NA,  # Dates non disponibles\n",
    "        'Company': df['Company'],\n",
    "        'Salary': df['Salary'],\n",
    "        'URL': df['Job link'],\n",
    "        'Skills': None  # Colonne à remplir ultérieurement\n",
    "    })\n",
    "    \n",
    "    # Ajout de la colonne Country\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        if any(mot in loc for mot in [\"remote\", \"hybrid\", \"united states\"]):\n",
    "            return \"United States\"\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Exécution et affichage\n",
    "dataset3 = traiter_dataset3()\n",
    "print(\"\\nAperçu du Dataset 3 (5 premières lignes):\")\n",
    "print(dataset3.head())\n",
    "print(\"\\nRépartition par pays:\")\n",
    "print(dataset3['Location'].value_counts())\n",
    "print(\"\\nStatistiques des valeurs manquantes:\")\n",
    "print(f\"- Dates: {dataset3['Date'].isna().sum()}/{len(dataset3)}\")\n",
    "print(f\"- Compétences: {dataset3['Skills'].isna().sum()}/{len(dataset3)}\")\n",
    "\n",
    "# Stockage en mémoire\n",
    "datasets_traites[\"dataset3\"] = dataset3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafa493",
   "metadata": {},
   "source": [
    "fourth dataset (1) :• ML Engineer Jobs – Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/arnabk123/data-science-data-analystand-ml-jobs-from-indeed?select=ML+Engineer+jobs+%28Indeed%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b96e0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu du Dataset 4_1 (5 premières lignes):\n",
      "     Job Title Description Location  Date  \\\n",
      "0  ML Engineer                India  <NA>   \n",
      "1  ML Engineer                India  <NA>   \n",
      "2  ML Engineer                India  <NA>   \n",
      "3  ML Engineer              Unknown  <NA>   \n",
      "4  ML Engineer                India  <NA>   \n",
      "\n",
      "                                           Company Salary URL  \\\n",
      "0                      LanceTech Solutions Pvt Ltd   <NA>       \n",
      "1                      LanceTech Solutions Pvt Ltd   <NA>       \n",
      "2                                           Google   <NA>       \n",
      "3                                 PMAM Corporation   <NA>       \n",
      "4  Mercedes-Benz Research and Development India...   <NA>       \n",
      "\n",
      "                            Skills  \n",
      "0                  Computer Vision  \n",
      "1                  Computer Vision  \n",
      "2        Deep Learning, Tensorflow  \n",
      "3                              NaN  \n",
      "4  Computer Vision, Neural Network  \n",
      "\n",
      "Répartition par pays:\n",
      "Location\n",
      "India      141\n",
      "Unknown     21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valeurs manquantes/par défaut:\n",
      "- Description: 162 / 162\n",
      "- Date: 162 / 162\n",
      "- URL: 162 / 162\n",
      "- Compétences renseignées: 104 / 162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def traiter_dataset4_1():\n",
    "    # Chargement des données\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\ML Engineer Jobs – Indeed\\ML Engineer jobs (Indeed).csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # Normalisation des noms de colonnes\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    \n",
    "    # Vérification des colonnes requises\n",
    "    colonnes_requises = ['job title', 'company', 'region', 'skills required']\n",
    "    manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "    if manquantes:\n",
    "        raise ValueError(f\"Colonnes manquantes : {manquantes}\")\n",
    "    \n",
    "    # Création du DataFrame standardisé\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['job title'],\n",
    "        'Description': '',  # Champ vide\n",
    "        'Location': df['region'],\n",
    "        'Date': pd.NA,  # Dates non disponibles\n",
    "        'Company': df['company'],\n",
    "        'Salary': pd.NA,  # Salaires non disponibles\n",
    "        'URL': '',  # URLs vides\n",
    "        'Skills': df['skills required']  # Seul dataset avec compétences\n",
    "    })\n",
    "    \n",
    "    # Détection du pays\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        villes_indiennes = [\"agra\", \"bengaluru\", \"hyderabad\", \"noida\", \"mumbai\", \n",
    "                          \"delhi\", \"pune\", \"chennai\", \"kolkata\", \"jaipur\"]\n",
    "        \n",
    "        if \"india\" in loc or \"remote\" in loc or \"hybrid\" in loc:\n",
    "            return \"India\"\n",
    "        if any(ville in loc for ville in villes_indiennes):\n",
    "            return \"India\"\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Exécution et affichage\n",
    "dataset4_1 = traiter_dataset4_1()\n",
    "print(\"\\nAperçu du Dataset 4_1 (5 premières lignes):\")\n",
    "print(dataset4_1.head())\n",
    "print(\"\\nRépartition par pays:\")\n",
    "print(dataset4_1['Location'].value_counts())\n",
    "print(\"\\nValeurs manquantes/par défaut:\")\n",
    "print(\"- Description:\", (dataset4_1['Description'] == '').sum(), \"/\", len(dataset4_1))\n",
    "print(\"- Date:\", dataset4_1['Date'].isna().sum(), \"/\", len(dataset4_1))\n",
    "print(\"- URL:\", (dataset4_1['URL'] == '').sum(), \"/\", len(dataset4_1))\n",
    "print(\"- Compétences renseignées:\", dataset4_1['Skills'].notna().sum(), \"/\", len(dataset4_1))\n",
    "\n",
    "# Stockage en mémoire\n",
    "datasets_traites[\"dataset4_1\"] = dataset4_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799975ee",
   "metadata": {},
   "source": [
    "fourth dataset (2) :• Data Analyst jobs – Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/arnabk123/data-science-data-analystand-ml-jobs-from-indeed?select=ML+Engineer+jobs+%28Indeed%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec4b02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 4_2 prêt \n",
      "      Job Title Description Location  Date                     Company Salary  \\\n",
      "0  Data analyst                India  <NA>                   BP Energy   <NA>   \n",
      "1  Data analyst                India  <NA>                     Arcadis   <NA>   \n",
      "2  Data analyst                India  <NA>  Scan Holdings Pvt. Limited   <NA>   \n",
      "3  Data analyst                India  <NA>                      BOEING   <NA>   \n",
      "4  Data analyst                India  <NA>     Boston Consulting Group   <NA>   \n",
      "\n",
      "  URL              Skills  \n",
      "0                     NaN  \n",
      "1      Big Data, Database  \n",
      "2                     NaN  \n",
      "3                     NaN  \n",
      "4                     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_dataset4_2():\n",
    "    \"\"\"\n",
    "    Prépare le dataset 4_2 en mémoire, avec 'India' dans Location, sans colonne Country.\n",
    "    \"\"\"\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\ML Engineer Jobs – Indeed\\Data Analyst jobs (Indeed).csv\"\n",
    "\n",
    "    try:\n",
    "        # Chargement des données\n",
    "        df = pd.read_csv(chemin)\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # Vérification des colonnes requises\n",
    "        colonnes_requises = ['job title', 'company', 'skills required']\n",
    "        manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "        if manquantes:\n",
    "            raise ValueError(f\"Colonnes manquantes : {manquantes}\")\n",
    "\n",
    "        # Création du DataFrame standardisé\n",
    "        df_final = pd.DataFrame({\n",
    "            'Job Title': df['job title'],\n",
    "            'Description': '',\n",
    "            'Location': 'India',  # Valeur fixe\n",
    "            'Date': pd.NA,\n",
    "            'Company': df['company'],\n",
    "            'Salary': pd.NA,\n",
    "            'URL': '',\n",
    "            'Skills': df['skills required']\n",
    "        })\n",
    "\n",
    "        print(\"✅ Dataset 4_2 prêt \")\n",
    "        print(df_final.head())\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur dans prepare_dataset4_2 : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "datasets_traites[\"dataset4_2\"] = prepare_dataset4_2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef792ad",
   "metadata": {},
   "source": [
    "fourth dataset (3) :• Data Analyst Jobs – Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/arnabk123/data-science-data-analystand-ml-jobs-from-indeed?select=ML+Engineer+jobs+%28Indeed%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f3dcdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 4_3 prêt (Location = 'India')\n",
      "      Job Title Description Location  Date                     Company Salary  \\\n",
      "0  Data analyst                India  <NA>                   BP Energy   <NA>   \n",
      "1  Data analyst                India  <NA>                     Arcadis   <NA>   \n",
      "2  Data analyst                India  <NA>  Scan Holdings Pvt. Limited   <NA>   \n",
      "3  Data analyst                India  <NA>                      BOEING   <NA>   \n",
      "4  Data analyst                India  <NA>     Boston Consulting Group   <NA>   \n",
      "\n",
      "  URL              Skills  \n",
      "0                     NaN  \n",
      "1      Big Data, Database  \n",
      "2                     NaN  \n",
      "3                     NaN  \n",
      "4                     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_dataset4_3():\n",
    "    \"\"\"\n",
    "    Prépare le dataset 4_3 en mémoire avec 'Location' fixé à 'India', sans colonne 'Country'.\n",
    "    \"\"\"\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\ML Engineer Jobs – Indeed\\Data Analyst jobs (Indeed).csv\"\n",
    "\n",
    "    try:\n",
    "        # Chargement des données\n",
    "        df = pd.read_csv(chemin)\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # Vérification des colonnes requises\n",
    "        colonnes_requises = ['job title', 'company', 'skills required']\n",
    "        manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "        if manquantes:\n",
    "            raise ValueError(f\"Colonnes manquantes : {manquantes}\")\n",
    "\n",
    "        # Création du DataFrame standardisé\n",
    "        df_final = pd.DataFrame({\n",
    "            'Job Title': df['job title'],\n",
    "            'Description': '',\n",
    "            'Location': 'India',  # Valeur fixe\n",
    "            'Date': pd.NA,\n",
    "            'Company': df['company'],\n",
    "            'Salary': pd.NA,\n",
    "            'URL': '',\n",
    "            'Skills': df['skills required']\n",
    "        })\n",
    "\n",
    "        print(\"✅ Dataset 4_3 prêt (Location = 'India')\")\n",
    "        print(df_final.head())\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur dans prepare_dataset4_3 : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "datasets_traites[\"dataset4_3\"] = prepare_dataset4_3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be108ee7",
   "metadata": {},
   "source": [
    "fifth dataset :Data-Science Job Postings & Skills (servira de lexique open source pour\n",
    "l’extraction des compétences)\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/asaniczka/data-science-job-postingsand-skills/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cec00dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 5 transformé en mémoire (Location → Country)\n",
      "🔎 Offres traitées: 12217\n",
      "🌍 Répartition par pays :\n",
      "Location\n",
      "United States     10163\n",
      "United Kingdom     1117\n",
      "Canada              634\n",
      "Australia           175\n",
      "Unknown             114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def format_date_to_iso(date_str):\n",
    "    \"\"\"Convertit une date en format ISO (YYYY-MM-DD).\"\"\"\n",
    "    if pd.isna(date_str) or str(date_str).strip() == '':\n",
    "        return None\n",
    "    try:\n",
    "        dt = pd.to_datetime(date_str, errors='coerce')\n",
    "        return dt.strftime('%Y-%m-%d') if not pd.isna(dt) else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def map_location_to_country(location):\n",
    "    \"\"\"Mappe un champ location à un pays.\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    loc = location.lower()\n",
    "\n",
    "    # Canada\n",
    "    if any(x in loc for x in [\"canada\", \"ontario\", \"quebec\", \"british columbia\", \"nova scotia\",\n",
    "                              \"alberta\", \"saskatchewan\", \"manitoba\", \"new brunswick\", \"newfoundland\",\n",
    "                              \"prince edward island\"]):\n",
    "        return \"Canada\"\n",
    "    \n",
    "    # United Kingdom\n",
    "    if any(x in loc for x in [\"england\", \"united kingdom\", \"scotland\", \"wales\",\n",
    "                              \"northern ireland\", \"london area\"]):\n",
    "        return \"United Kingdom\"\n",
    "    \n",
    "    # Australia\n",
    "    if any(x in loc for x in [\"australia\", \"queensland\", \"victoria\", \"new south wales\",\n",
    "                              \"australian capital territory\", \"western australia\", \"darwin\"]):\n",
    "        return \"Australia\"\n",
    "\n",
    "    # Mexico\n",
    "    if any(x in loc for x in [\"mexico\", \"baja california\"]):\n",
    "        return \"Mexico\"\n",
    "\n",
    "    # Italy\n",
    "    if any(x in loc for x in [\"italy\", \"lombardy\"]):\n",
    "        return \"Italy\"\n",
    "\n",
    "    # United States\n",
    "    us_states = [\"united states\", \"ca\", \"ny\", \"tx\", \"fl\", \"wa\", \"pa\", \"va\", \"nc\", \"oh\", \"il\", \"nj\", \"az\", \"ga\",\n",
    "                 \"or\", \"co\", \"mi\", \"mn\", \"wi\", \"sc\", \"in\", \"mo\", \"ok\", \"ky\", \"al\", \"tn\", \"la\", \"nv\", \"ar\", \"ut\",\n",
    "                 \"nm\", \"ia\", \"ks\", \"ct\", \"ma\", \"ri\", \"vt\", \"nh\", \"me\", \"mt\", \"nd\", \"sd\", \"ne\", \"id\", \"wv\", \"wy\",\n",
    "                 \"ak\", \"hi\"]\n",
    "    if any(state in loc for state in us_states):\n",
    "        return \"United States\"\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def transform_dataset5_memory(input_path1, input_path2):\n",
    "    try:\n",
    "        # Chargement des fichiers\n",
    "        df_main = pd.read_csv(input_path1)\n",
    "        df_skills = pd.read_csv(input_path2)\n",
    "        \n",
    "        # Format de date\n",
    "        df_main['formatted_date'] = df_main['last_processed_time'].apply(format_date_to_iso)\n",
    "        \n",
    "        # Agrégation des compétences par job_link\n",
    "        skills_agg = df_skills.groupby('job_link')['job_skills'].agg(\n",
    "            lambda x: ', '.join([str(s) for s in x if pd.notna(s)])\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Fusion des deux dataframes\n",
    "        df_final = pd.merge(df_main, skills_agg, on='job_link', how='left')\n",
    "        df_final.columns = df_final.columns.str.lower().str.strip()\n",
    "        \n",
    "        # Construction du DataFrame final au format standard\n",
    "        new_df = pd.DataFrame({\n",
    "            'Job Title': df_final['job_title'],\n",
    "            'Description': '',\n",
    "            'Location': df_final['job_location'],\n",
    "            'Date': df_final['formatted_date'],\n",
    "            'Company': df_final['company'],\n",
    "            'Salary': np.nan,\n",
    "            'URL': df_final['job_link'],\n",
    "            'Skills': df_final['job_skills']\n",
    "        })\n",
    "\n",
    "        # Remplacement de 'Location' par le pays détecté\n",
    "        new_df['Location'] = new_df['Location'].apply(map_location_to_country)\n",
    "\n",
    "        print(\"✅ Dataset 5 transformé en mémoire (Location → Country)\")\n",
    "        print(f\"🔎 Offres traitées: {len(new_df)}\")\n",
    "        print(f\"🌍 Répartition par pays :\\n{new_df['Location'].value_counts().head()}\")\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur dans transform_dataset5_memory : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "datasets_traites[\"dataset5\"] = transform_dataset5_memory(\n",
    "    r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science Job Postings & Skills\\job_postings.csv\",\n",
    "    r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science Job Postings & Skills\\job_skills.csv\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "825ba0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 6 préparé en mémoire\n",
      "Location     \n",
      "United States    4631\n",
      "Puerto Rico        46\n",
      "Unknown            22\n",
      "Canada              5\n",
      "Australia           4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_dataset6():\n",
    "    input_file = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\scrapped_jobs_api.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    def extract_country(location):\n",
    "        if pd.isna(location):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        loc = location.lower()\n",
    "\n",
    "        if \"puerto rico\" in loc or \"pr\" in loc:\n",
    "            return \"Puerto Rico\"\n",
    "        \n",
    "        if any(x in loc for x in [\"canada\", \"ontario\", \"british columbia\", \"alberta\", \"quebec\", \"nova scotia\", \"manitoba\", \"new brunswick\"]):\n",
    "            return \"Canada\"\n",
    "        \n",
    "        if any(x in loc for x in [\"united kingdom\", \"england\", \"scotland\", \"wales\", \"london\", \"northern ireland\"]):\n",
    "            return \"United Kingdom\"\n",
    "        \n",
    "        if any(x in loc for x in [\"australia\", \"queensland\", \"victoria\", \"new south wales\", \"australian capital territory\", \"nsw\", \"melbourne\", \"sydney\"]):\n",
    "            return \"Australia\"\n",
    "        \n",
    "        if any(x in loc for x in [\"mexico\", \"baja california\"]):\n",
    "            return \"Mexico\"\n",
    "\n",
    "        if any(x in loc for x in [\n",
    "            \"united states\", \"usa\", \"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"dc\", \"de\", \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\",\n",
    "            \"ks\", \"ky\", \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\", \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\",\n",
    "            \"ok\", \"or\", \"pa\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"\n",
    "        ]) or \",\" in loc:\n",
    "            return \"United States\"\n",
    "\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Appliquer la détection de pays\n",
    "    df['Location'] = df['Location'].apply(extract_country)\n",
    "\n",
    "    # Création du DataFrame final au format commun\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['Title'] if 'Title' in df.columns else np.nan,\n",
    "        'Description': '',\n",
    "        'Location': df['Location'],\n",
    "        'Date': df['DatePosted'] if 'DatePosted' in df.columns else np.nan,\n",
    "        'Company': df['Company'] if 'Company' in df.columns else np.nan,\n",
    "        'Salary': np.nan,\n",
    "        'URL': df['URL'] if 'URL' in df.columns else np.nan,\n",
    "        'Skills': ''\n",
    "    })\n",
    "\n",
    "    print(\"✅ Dataset 6 préparé en mémoire\")\n",
    "    print(df_final[['Location']].value_counts().head())\n",
    "\n",
    "    return df_final\n",
    "datasets_traites[\"dataset6\"] = prepare_dataset6()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ff892",
   "metadata": {},
   "source": [
    "join dataset et traitement du salary et push dans mongodb compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eeb4999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Fusion des datasets en mémoire...\n",
      "🧹 Normalisation des salaires...\n",
      "✅ Salaires normalisés\n",
      "🕒 Normalisation des dates au format 'DD-MM-YYYY'...\n",
      "✅ Dates normalisées\n",
      "📊 Aperçu du jeu de données fusionné :\n",
      "                                           Job Title  \\\n",
      "0  newArtificial intelligence security specialist...   \n",
      "1                              newEDA Tool Developer   \n",
      "2  Junior Project Engineer - Artificial Intellige...   \n",
      "3                                          Test Lead   \n",
      "4                                     + Lead Testers   \n",
      "\n",
      "                                         Description Location Date Company  \\\n",
      "0  An artificial intelligence (AI) specialist app...    India  NaN           \n",
      "1  We are seeking highly motivated individuals wi...    India  NaN           \n",
      "2  We are dedicated to providing quality and effe...  Unknown  NaN           \n",
      "3  Working on over 18 different browser/os/ mobil...  Unknown  NaN           \n",
      "4  Total Experience: 6- 8 yrs.\\nAutomation Testin...    India  NaN           \n",
      "\n",
      "   Salary URL Skills  \n",
      "0     NaN       None  \n",
      "1     NaN       None  \n",
      "2     NaN       None  \n",
      "3     NaN       None  \n",
      "4     NaN       None  \n",
      "\n",
      "📡 Connexion à MongoDB Compass...\n",
      "✅ 21280 documents insérés avec succès dans MongoDB !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# --- 1. Normalisation des salaires ---\n",
    "def normalize_salary(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s_clean = s.replace(',', '').replace('CA$', '').replace('USD', '').replace('$', '').strip()\n",
    "    match = re.findall(r'(\\d+\\.?\\d*)\\s*[-–]?\\s*(\\d+\\.?\\d*)?\\s*(a year|yearly|a month|a week|an hour|a day|hr|mo|yr|hour|monthly|daily)?', s_clean.lower())\n",
    "    if not match:\n",
    "        return np.nan\n",
    "    try:\n",
    "        min_val = float(match[0][0])\n",
    "        max_val = float(match[0][1]) if match[0][1] else min_val\n",
    "        avg_val = (min_val + max_val) / 2\n",
    "        period = match[0][2]\n",
    "        if \"hour\" in period or \"hr\" in period:\n",
    "            return avg_val * 40 * 52\n",
    "        elif \"week\" in period:\n",
    "            return avg_val * 52\n",
    "        elif \"month\" in period or \"mo\" in period:\n",
    "            return avg_val * 12\n",
    "        elif \"day\" in period or \"daily\" in period:\n",
    "            return avg_val * 5 * 52\n",
    "        else:\n",
    "            return avg_val\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# --- 2. Pipeline principal ---\n",
    "def pipeline_in_memory(datasets_dict):\n",
    "    print(\"🔄 Fusion des datasets en mémoire...\")\n",
    "    df_final = pd.concat(datasets_dict.values(), ignore_index=True)\n",
    "\n",
    "    print(\"🧹 Normalisation des salaires...\")\n",
    "    df_final[\"Salary\"] = df_final[\"Salary\"].apply(normalize_salary)\n",
    "    print(\"✅ Salaires normalisés\")\n",
    "\n",
    "    # 🔧 Normalisation des dates\n",
    "    print(\"🕒 Normalisation des dates au format 'DD-MM-YYYY'...\")\n",
    "    df_final[\"Date\"] = pd.to_datetime(df_final[\"Date\"], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "    print(\"✅ Dates normalisées\")\n",
    "    print(\"📊 Aperçu du jeu de données fusionné :\")\n",
    "    print(df_final.head())\n",
    "\n",
    "    print(\"\\n📡 Connexion à MongoDB Compass...\")\n",
    "    try:\n",
    "        client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        db = client[\"job_database\"]\n",
    "        collection = db[\"job_offers\"]\n",
    "\n",
    "        # Nettoyer la collection avant insertion (optionnel mais conseillé)\n",
    "        collection.delete_many({})\n",
    "\n",
    "        # Insertion\n",
    "        records = df_final.to_dict(orient=\"records\")\n",
    "        collection.insert_many(records)\n",
    "        print(f\"✅ {len(records)} documents insérés avec succès dans MongoDB !\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l'insertion dans MongoDB: {e}\")\n",
    "\n",
    "# --- 3. Lancement ---\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline_in_memory(datasets_traites)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3e7d1",
   "metadata": {},
   "source": [
    "job skills dataset (relational database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01f754b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final CSV saved as 'job_data_cleaned_final.csv'\n",
      "✅ Données insérées dans MongoDB Compass (collection: job_offers)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load CSVs\n",
    "company_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\company_dim.csv\")\n",
    "job_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\job_postings_fact.csv\")\n",
    "skills_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\skills_dim.csv\")\n",
    "skills_job_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\skills_job_dim.csv\")\n",
    "\n",
    "# Merge job postings with companies\n",
    "job_with_company = pd.merge(job_df, company_df, on=\"company_id\", how=\"left\")\n",
    "\n",
    "# Merge skills with their IDs\n",
    "skills_full = pd.merge(skills_job_df, skills_df, on=\"skill_id\", how=\"left\")\n",
    "\n",
    "# Merge everything by job_id\n",
    "final_df = pd.merge(job_with_company, skills_full, on=\"job_id\", how=\"left\")\n",
    "\n",
    "# Group by job_id and aggregate\n",
    "cleaned_df = final_df.groupby(\"job_id\").agg({\n",
    "    \"job_title_short\": \"first\",\n",
    "    \"job_posted_date\": \"first\",\n",
    "    \"job_country\": \"first\",\n",
    "    \"name\": \"first\",\n",
    "    \"salary_year_avg\": \"first\",\n",
    "    \"link\": \"first\",\n",
    "    \"skills\": lambda x: ', '.join(sorted(set(filter(pd.notna, x))))\n",
    "}).reset_index()\n",
    "\n",
    "# Format date\n",
    "cleaned_df[\"Date\"] = pd.to_datetime(cleaned_df[\"job_posted_date\"], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# Add Description as NaN\n",
    "cleaned_df[\"Description\"] = pd.NA\n",
    "\n",
    "# Rename columns\n",
    "cleaned_df = cleaned_df.rename(columns={\n",
    "    \"job_title_short\": \"Job Title\",\n",
    "    \"job_country\": \"Location\",\n",
    "    \"name\": \"Company\",\n",
    "    \"salary_year_avg\": \"Salary\",\n",
    "    \"link\": \"URL\",\n",
    "    \"skills\": \"Skills\"\n",
    "})\n",
    "\n",
    "# Reorder columns\n",
    "final_columns = [\"Job Title\", \"Description\", \"Location\", \"Date\", \"Company\", \"Salary\", \"URL\", \"Skills\"]\n",
    "cleaned_df = cleaned_df[final_columns]\n",
    "\n",
    "# Save to CSV\n",
    "cleaned_df.to_csv(\"job_data_cleaned_final.csv\", index=False)\n",
    "print(\"✅ Final CSV saved as 'job_data_cleaned_final.csv'\")\n",
    "\n",
    "# Push to MongoDB Compass\n",
    "try:\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"job_database\"]\n",
    "    collection = db[\"job_offers\"]\n",
    "\n",
    "    # Convert DataFrame to dictionary and insert into MongoDB\n",
    "    collection.insert_many(cleaned_df.to_dict(orient=\"records\"))\n",
    "    print(\"✅ Données insérées dans MongoDB Compass (collection: job_offers)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de l'insertion MongoDB : {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
